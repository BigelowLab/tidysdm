---
title: "Example of additional tidymodels features"
output: rmarkdown::html_vignette
#output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{Example of additional tidymodels features}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Additional features of `tidymodels`

In this vignette, we illustrate how a number of features from `tidymodels` can
be used to enhance SDMs. We reccomend users first become famliar with `tidymodels`;
there are a number of excellent tutorials on its dedicated [website](https://www.tidymodels.org/)
We reuse the example on the Iberian lizard that we used
to give an overview of the features of `tidysdm`.

# Preparing your data

Here we quickly shape the dataset as we did in the overview; see that vignette
for a detailed explantion of the steps.

We start by reading in a set of presences for a species of lizard that inhabits
the Iberian peninsula, *Lacerta schreiberi*, and cast it as an `sf` object
```{r load_presences}
library(tidysdm)
data(lacerta)
library(sf)
lacerta <- st_as_sf(lacerta, coords = c("longitude","latitude"))
st_crs(lacerta) = 4326
```

Get the appropriate land mask

```{r land_mask, eval=FALSE}
library(pastclim)
land_mask <-
  get_land_mask(time_ce = 1985, dataset = "WorldClim_2.1_10m")

# Iberia peninsula extension
iberia_poly <-
  terra::vect(
    "POLYGON((-9.8 43.3,-7.8 44.1,-2.0 43.7,3.6 42.5,3.8 41.5,1.3 40.8,0.3 39.5,
     0.9 38.6,-0.4 37.5,-1.6 36.7,-2.3 36.3,-4.1 36.4,-4.5 36.4,-5.0 36.1,
    -5.6 36.0,-6.3 36.0,-7.1 36.9,-9.5 36.6,-9.4 38.0,-10.6 38.9,-9.5 40.8,
    -9.8 43.3))"
  )

crs(iberia_poly) <- "lonlat"
# crop the extent
land_mask <- crop(land_mask, iberia_poly)
# and mask to the polygon
land_mask <- mask(land_mask, iberia_poly)
```

```{r echo=FALSE, results='hide'}
library(pastclim)
set_data_path(on_CRAN = TRUE)
# Iberia peninsula extension
iberia_poly <- terra::vect("POLYGON((-9.8 43.3,-7.8 44.1,-2.0 43.7,3.6 42.5,3.8 41.5,1.3 40.8,0.3 39.5,0.9 38.6,-0.4 37.5,-1.6 36.7,-2.3 36.3,-4.1 36.4,-4.5 36.4,-5.0 36.1,-5.6 36.0,-6.3 36.0,-7.1 36.9,-9.5 36.6,-9.4 38.0,-10.6 38.9,-9.5 40.8,-9.8 43.3))")

crs(iberia_poly)<-"lonlat"
gdal(warn=3)
land_mask<-rast(system.file("extdata/lacerta_land_mask.nc", package="tidysdm"))
```

and then thin the presences (we reset the seed before each operation to match
what we had in the original vignette):
```{r thin}
set.seed(1234567)
lacerta<-thin_by_cell(lacerta, raster = land_mask)
set.seed(1234567)
lacerta_thin<-thin_by_dist(lacerta, dist_min = km2m(20))
set.seed(1234567)
lacerta_thin <- sample_pseudoabs(lacerta_thin, 
                               n = 3 * nrow(lacerta_thin), 
                               raster = land_mask,
                               method = c("dist_min", km2m(50)))
```

Quickly plot them:
```{r plot_sample_pseudoabs, fig.width=6, fig.height=4}
library(tidyterra)
ggplot() +
  geom_spatraster(data=land_mask, aes(fill=land_mask_1985))+
  geom_sf(data = lacerta_thin, aes(col = class))

```

Let's extract the climate for the variables of interest

```{r eval=FALSE}
download_dataset("WorldClim_2.1_10m")
climate_present<-pastclim::region_slice(time_ce = 1985, 
                                        bio_variables = climate_vars, 
                                        data="WorldClim_2.1_10m", 
                                        crop=iberia_poly)
```

```{r echo=FALSE, results='hide'}
gdal(warn=3)
climate_present<-rast(system.file("extdata/lacerta_climate_present_10m.nc", package="tidysdm"))
climate_vars <- names(climate_present)
```


```{r}
lacerta_thin <- lacerta_thin %>% 
  bind_cols(terra::extract(climate_present, lacerta_thin, ID=FALSE))
```

# The initial split

The standard approach in `tidymodels` is to make an initial split of the data
into a test and a training set. We will use retain 20% of the data (1/5) for the testing set, and
use the rest for training.

```{r initial_split}
set.seed(1005)
lacerta_initial <- spatial_initial_split(lacerta_thin, prop = 1/5, spatial_block_cv)
autoplot(lacerta_initial)
```

# Fit the model to the training set, tuning by crossvalidation

We can now extract the training set from our `lacerta_initial` split, and sample
folds to set up crossvalidation (we use the same grid we used on the full
dataset `arabiensis` for the `initial_split`)
```{r training_cv}
set.seed(1005)
lacerta_training <- training(lacerta_initial)
lacerta_cv <- spatial_block_cv(lacerta_training, v = 5,
                                cellsize = grid_cellsize(lacerta_thin),
                                offset = grid_offset(lacerta))
autoplot(lacerta_cv)
```

Only certain type of models (e.g. glm, mars) struggle with correlated variables;
other algorithms, such as random forests, can handle correlated variables. So,
we will create two recipes, one with all variables, and one only with the
variables that are uncorrelated:

```{r recipe}
lacerta_rec_all <- recipe(lacerta_thin, formula=class~.)
lacerta_rec_uncor <- lacerta_rec_all %>% step_select(all_of(c("class","bio05","bio06", "bio13", "bio15")))
lacerta_rec_uncor <- lacerta_rec_all %>% step_rm(all_of( c("bio01", "bio02", "bio03", "bio04", "bio07", "bio08", "bio09", "bio10", "bio11", "bio12", "bio14", "bio16", "bio17", "bio18", "bio19", "altitude")))


lacerta_rec_uncor
```

And now use this in a `workflowset` (we will keep it small for computational time),
selecting the appropriate recipe for each model. We will include a model (multivariate
adaptive regression splines, or MARS) which does not have a wrapper in `tidysdm`
for creating a model specificaiton. However, we can use a standard model spec from
`yardstick`:

```{r workflow_set}
lacerta_models <-
  # create the workflow_set
  workflow_set(
    preproc = list(uncor = lacerta_rec_uncor, # recipe for the glm
                   all = lacerta_rec_all,  # recipe for the random forest
                   all = lacerta_rec_uncor # recipe for mars
    ),
    models = list(
      # the standard glm specs
      glm = sdm_spec_glm(),
      # rf specs with tuning
      rf = sdm_spec_rf(),
      # mars specs with tuning
      mars = parsnip::mars(num_terms=tune()) %>% 
        parsnip::set_engine("earth") %>%
        parsnip::set_mode("classification")
    ),
    # make all combinations of preproc and models,
    cross = FALSE
  ) %>%
  # tweak controls to store information needed later to create the ensemble
  # note that we use the bayes version as we will use a Bayes search (see later)
  option_add(control = stacks::control_stack_bayes())
```


We can now use the block CV folds to
tune and assess the models. Note that there are multiple tuning approaches,
besides the standard grid method. Here we will use `tune_bayes`
(to keep computations fast, we will only explore 3
combination of hypeparameters per model; that far too little in real life!).

This tuning method (as opposed to use a standard grid) does not allow for
hyperparameters with unknown limits, but
`mtry` for random forest and gbm is undefined as its upper range depends on the number of 
variables in the dataset. So, before tuning, we need to finalise `mtry` by 
informing the set dials with the actual data:

```{r}
rf_param <- lacerta_models %>%
  # extract the rf workflow
  extract_workflow ("all_rf") %>%
  # extract its parameters dials (used to tune)
  extract_parameter_set_dials() %>%
  # give it the predictors to finalize mtry
  finalize(x=st_drop_geometry(lacerta_thin) %>% select(-class))

# now update the workflowset with the new parameter info
lacerta_models <- lacerta_models %>%
  option_add(param_info=rf_param,id="all_rf")

```


And now we can tune the models:
```{r tune_grid}
set.seed(1234567)
lacerta_models <- 
   lacerta_models %>% 
   workflow_map("tune_bayes", resamples = lacerta_cv, initial=8,
                metrics = sdm_metric_set(), verbose = TRUE)
```

We can have a look at the performance of our models with:

```{r}
autoplot(lacerta_models)
```

Not let's stack the models to build an ensemble

```{r build_stack}
library(stacks)
set.seed(1005)
lacerta_stack <- 
  # initialize the stack
  stacks() %>%
  # add candidate members
  add_candidates(lacerta_models) %>%
  # determine how to combine their predictions
  blend_predictions() %>%
  # fit the candidates with nonzero weights (i.e.nonzero stacking coefficients)
  fit_members()

autoplot(lacerta_stack, type = "weights")
```

And finally, we can make predictions on the testing data with:

```{r predict_test}
lacerta_testing <- testing(lacerta_initial)

lacerta_test_pred <- 
  lacerta_testing %>%
  bind_cols(predict(lacerta_stack, ., type="prob"))
```

And look at the goodness of fit with AUC

```{r assess_test}
roc_auc(data= lacerta_test_pred,truth=class,.pred_presence)
```

# Projecting to the present
We can now make predictions with this ensemble (using the default option of taking
the mean of the predictions from each model).

```{r plot_present, fig.width=6, fig.height=4}
prediction_present <- predict_raster(lacerta_stack, climate_present, type="prob")
ggplot() +
  geom_spatraster(data=prediction_present, aes(fill=.pred_presence))+
  scale_fill_terrain_c() +
  # plot presences used in the model
  geom_sf(data = lacerta_thin %>% filter(class=="presence"))
```
