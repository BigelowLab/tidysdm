---
title: "Example of additional tidymodels features"
output: rmarkdown::html_vignette
#output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{Example of additional tidymodels features}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Troubleshooting models that fail

In this vignette, we illustrate how to troubleshoot tuning errors. This is not a 
comprehensive list, but rather an attempt to illustrate how an error 
can be approached. The major difficulty is that, within a `workflowset`, error
messages for a given  can be difficult to see 

# list of things to do
a) check that a recipe works by prepping it and baking it to see what would
be passed to the model:
```{r}
lacerta_rec_uncor %>% prep(training=lacerta_thin) ->foo
```

b) try fitting the workflow to the full dataset (i.e. get out of workflowsets
and tuning)


c) try tuning the individual workflow, and then fit to the specific split that
is giving a problem



b) make sure you don't have missing data
```{r}
recipe(Ozone ~ ., data = airquality) %>%
  step_naomit() %>%
  prep(airquality, verbose = FALSE) %>%
  bake(new_data = NULL)

step_na_omit()
```



# Preparing your data

Here we quickly shape the dataset as we did in the overview; see that vignette
for a detailed explanation of the steps.

We start by reading in a set of presences for a species of lizard that inhabits
the Iberian peninsula, *Lacerta schreiberi*, and cast it as an `sf` object
```{r load_presences}
library(tidysdm)
data(lacerta)
library(sf)
lacerta <- st_as_sf(lacerta, coords = c("longitude","latitude"))
st_crs(lacerta) = 4326
```

Get the appropriate land mask

```{r land_mask, eval=FALSE}
library(pastclim)
land_mask <-
  get_land_mask(time_ce = 1985, dataset = "WorldClim_2.1_10m")

# Iberia peninsula extension
iberia_poly <-
  terra::vect(
    "POLYGON((-9.8 43.3,-7.8 44.1,-2.0 43.7,3.6 42.5,3.8 41.5,1.3 40.8,0.3 39.5,
     0.9 38.6,-0.4 37.5,-1.6 36.7,-2.3 36.3,-4.1 36.4,-4.5 36.4,-5.0 36.1,
    -5.6 36.0,-6.3 36.0,-7.1 36.9,-9.5 36.6,-9.4 38.0,-10.6 38.9,-9.5 40.8,
    -9.8 43.3))"
  )

crs(iberia_poly) <- "lonlat"
# crop the extent
land_mask <- crop(land_mask, iberia_poly)
# and mask to the polygon
land_mask <- mask(land_mask, iberia_poly)
```

```{r echo=FALSE, results='hide'}
library(pastclim)
set_data_path(on_CRAN = TRUE)
# Iberia peninsula extension
iberia_poly <- terra::vect("POLYGON((-9.8 43.3,-7.8 44.1,-2.0 43.7,3.6 42.5,3.8 41.5,1.3 40.8,0.3 39.5,0.9 38.6,-0.4 37.5,-1.6 36.7,-2.3 36.3,-4.1 36.4,-4.5 36.4,-5.0 36.1,-5.6 36.0,-6.3 36.0,-7.1 36.9,-9.5 36.6,-9.4 38.0,-10.6 38.9,-9.5 40.8,-9.8 43.3))")

crs(iberia_poly)<-"lonlat"
gdal(warn=3)
land_mask<-rast(system.file("extdata/lacerta_land_mask.nc", package="tidysdm"))
```

and then thin the presences (we reset the seed before each operation to match
what we had in the original vignette):
```{r thin}
set.seed(1234567)
lacerta<-thin_by_cell(lacerta, raster = land_mask)
set.seed(1234567)
lacerta_thin<-thin_by_dist(lacerta, dist_min = km2m(20))
set.seed(1234567)
lacerta_thin <- sample_pseudoabs(lacerta_thin, 
                               n = 3 * nrow(lacerta_thin), 
                               raster = land_mask,
                               method = c("dist_min", km2m(50)))
```

Quickly plot them:
```{r plot_sample_pseudoabs, fig.width=6, fig.height=4}
library(tidyterra)
ggplot() +
  geom_spatraster(data=land_mask, aes(fill=land_mask_1985))+
  geom_sf(data = lacerta_thin, aes(col = class))

```

Let's extract the climate for the variables of interest

```{r eval=FALSE}
download_dataset("WorldClim_2.1_10m")
climate_present<-pastclim::region_slice(time_ce = 1985, 
                                        bio_variables = climate_vars, 
                                        data="WorldClim_2.1_10m", 
                                        crop=iberia_poly)
```

```{r echo=FALSE, results='hide'}
gdal(warn=3)
climate_present<-rast(system.file("extdata/lacerta_climate_present_10m.nc", package="tidysdm"))
climate_vars <- names(climate_present)
```


```{r}
lacerta_thin <- lacerta_thin %>% 
  bind_cols(terra::extract(climate_present, lacerta_thin, ID=FALSE))
```

# Fit the model by crossvalidation

Only certain type of models (e.g. glm, gams) struggle with correlated variables;
other algorithms, such as random forests, can handle correlated variables. So,
we will create two recipes, one with all variables, and one only with the
variables that are uncorrelated:

```{r recipe}
lacerta_rec_all <- recipe(lacerta_thin, formula=class~.)
lacerta_rec_uncor <- lacerta_rec_all %>% step_select(bio05,bio06, bio13, bio15)
lacerta_rec_uncor <- lacerta_rec_all %>% step_select(bio05,bio06)

lacerta_rec_uncor
```

And now use this in a `workflowset` (we will keep it small for computational time),
selection the appropriate recipe for each model:

```{r workflow_set}
lacerta_models <-
  # create the workflow_set
  workflow_set(
    preproc = list(uncor = lacerta_rec_uncor, # recipe for the glm
                   all = lacerta_rec_all,  # recipe for the random forest
                   all = lacerta_rec_all),
    models = list(
      # the standard glm specs
      glm = sdm_spec_glm(),
      # rf specs with tuning
      rf = sdm_spec_rf(),
      # boosted tree model (gbm) specs with tuning
      gbm = sdm_spec_boost_tree()
    ),
    # make all combinations of preproc and models,
    cross = FALSE
  ) %>%
  # tweak controls to store information needed later to create the ensemble
  option_add(control = control_ensemble_grid())
```


We now want to set up a spatial block cross-validation scheme to tune and assess
our models. We will do an 80:20 split, by creating 5 folds.

```{r training_cv, fig.width=6, fig.height=4}
library(tidysdm)
set.seed(100)
lacerta_cv <- spatial_block_cv(lacerta_thin, v = 5)
autoplot(lacerta_cv)
```

We can now use the block CV folds to
tune and assess the models (to keep computations fast, we will only explore 3
combination of hypeparameters per model; that far too little in real life!):
```{r tune_grid}
set.seed(1234567)
lacerta_models <- 
   lacerta_models %>% 
   workflow_map("tune_grid", resamples = lacerta_cv, grid = 3, 
                metrics = sdm_metric_set(), verbose = TRUE)
```








We remove the ID column, as it is not relevant:
```{r clean_data}
arabiensis <- arabiensis %>% dplyr::select(-ID)
```

We now need to extract climatic variables for this species. We use the
present reconstructions from `pastclim`, subsetted to the region of interest
(Sub Saharan Africa).

```{r load_climate}
library(pastclim)

# climate variables to consider
climate_vars <- c("bio01","bio04", "bio05","bio06", "bio07", "bio08", "bio09", "bio10", "bio11", "bio12", "bio13", "bio14", "bio16", "bio18","bio17", "bio19","npp", "lai", "rugosity")

# SubSaharan Africa
sub_s_africa <- terra::vect("POLYGON((-19.36 22.12,38.17 22.10,38.96 19.53,
                          40.76 16.98,43.71 12.12,52.36 13.59,54.30 7.03,
                          30.39 -34.59,15.28 -36.31,-19.18 13.59,-19.36 22.12))")

climate_present<-pastclim::region_slice(time_bp = 0, 
                                        bio_variables = climate_vars, 
                                        data="Beyer2020", 
                                        crop=sub_s_africa)
```

Environmental variables are often highly correlated, and collinearity is an issue
for several types of models. Subset to variables with less than 0.7 correlation

```{r choose_var_cor}
vars_to_keep <- filter_high_cor(climate_present, cutoff = 0.7)
vars_to_keep
```

Subset the raster to only include these variables (it would be better to use a bit of biology):

```{r}
climate_present<-climate_present[[vars_to_keep]]
```
Note that we could do the above in a `tidy` way using `tidyterra`. However,
using native `terra` commands tends to be more efficient, which can be important
when using large rasters.

Convert our dataset into an `sf` data.frame so that we can easily plot it
(here `tidyterra` shines):
```{r cast to sf}
library(sf)
arabiensis <- st_as_sf(arabiensis, coords = c("longitude","latitude"))
st_crs(arabiensis) = 4326
library(tidyterra)
ggplot() +
  geom_spatraster(data=climate_present, aes(fill=bio01))+
  scale_fill_terrain_c() +
  geom_sf(data = arabiensis)


```

Now thin the observations to have a one per cell in the raster (it would be better
if we had an equal area projection...), and remove locations outside the
desired area (e.g. Madagascar):
```{r thin_by_cell}
set.seed(123)
arabiensis<-thin_by_cell(arabiensis, raster = climate_present)
nrow(arabiensis)
```

Now thin further to remove points that are closer than 70km (note that
the standard map units for a lonlat projection are m!):
```{r thin_by_dist}
set.seed(123)
arabiensis<-thin_by_dist(arabiensis, dist_min = 70000)
nrow(arabiensis)
```

Let's see what we have left of our points:
```{r}
ggplot() +
  geom_spatraster(data=climate_present, aes(fill=bio01))+
  scale_fill_terrain_c() +
  geom_sf(data = arabiensis)
```

Now sample pseudoabsences (we will constraint them to be at least 70km away
from any presences), selecting as many points as presences:
```{r}
set.seed(123)
arabiensis <- sample_pseudoabs(arabiensis, 
                               n=nrow(arabiensis), 
                               raster=climate_present,
                               method=c("dist_min", 70000))
```

Let's see our presences and absences:
```{r}
ggplot() +
  geom_spatraster(data=climate_present, aes(fill=bio01))+
  scale_fill_terrain_c() +
  geom_sf(data = arabiensis, aes(col = class))
```

Now let's get the climate for these location
```{r climate_for_locations}
arabiensis <- arabiensis %>% 
  bind_cols(extract(climate_present, arabiensis, ID=FALSE))
```

# The initial split

Ok, we are now ready to split our data. It is wise to set the seed of the random
number generator every time we do any sampling, so that our code is fully 
repeatable. We will use retain 20% of the data (1/5) for the testing set, and
use the rest for training.

```{r initial_split}
library(tidysdm)
set.seed(1005)
arabiensis_initial <- spatial_initial_split(arabiensis, prop = 1/5, spatial_block_cv)
autoplot(arabiensis_initial)
```

# Fit the model to the training set

We can now extract the training set from our `arabiensis_initial` split, and sample
folds to set up crossvalidation (we use the same grid we used on the full
dataset `arabiensis` for the `initial_split`)
```{r training_cv}
set.seed(1005)
arabiensis_training <- training(arabiensis_initial)
arabiensis_cv <- spatial_block_cv(arabiensis_training, v = 5,
                                cellsize = grid_cellsize(arabiensis),
                                offset = grid_offset(arabiensis))
autoplot(arabiensis_cv)
```

Next, we need to set up a `recipe` (*class* is the outcome,
all other variables are predictors; note that, for `sf` objects, `geometry` is
automatically ignored as a predictor):
```{r recipe}
arabiensis_rec <- recipe(arabiensis_training, formula=class~.)
arabiensis_rec
```

Build a `workflow_set` with our models, defining which hyperparameters we want to
tune.

```{r workflow_set}
arabiensis_models <-
  # create the workflow_set
  workflow_set(
    preproc = list(default = arabiensis_rec),
    models = list(
      # the standard glm specs
      glm = sdm_spec_glm(),
      # the standard sdm specs
      gam = sdm_spec_gam(),
      # rf specs with tuning
      rf = sdm_spec_rf()
    ),
    # make all combinations of preproc and models,
    cross = TRUE
  ) %>%
  # set formula for gams
  update_workflow_model("default_gam",
                        spec = sdm_spec_gam(),
                        formula = gam_formula(arabiensis_rec)) %>%
  # tweak controls to store information needed later for stacking
  option_add(control = control_ensemble_grid())
```


We can now use the block CV folds we built earlier from the training dataset to
tune and assess the models:
```{r tune_grid}
set.seed(1005)
arabiensis_models <- 
   arabiensis_models %>% 
   workflow_map("tune_grid", resamples = arabiensis_cv, grid = 5, 
                metrics = metric_set(roc_auc), verbose = TRUE)
```

Note that `workflow_set` correctly detects that we have no tuning parameters for 
*glm* and *gam*. We can have a look at the performance of our models with:

```{r}
autoplot(arabiensis_models)
```

Not let's stack the models to build an ensemble

```{r build_stack}
set.seed(1005)
arabiensis_stack <- 
  # initialize the stack
  stacks() %>%
  # add candidate members
  add_candidates(arabiensis_models) %>%
  # determine how to combine their predictions
  blend_predictions() %>%
  # fit the candidates with nonzero weights (i.e.nonzero stacking coefficients)
  fit_members()

autoplot(arabiensis_stack, type = "weights")
```

And finally, we can make predictions on the testing data with:

```{r predict_test}
arabiensis_testing <- testing(arabiensis_initial)
arabiensis_test_pred <- 
  arabiensis_testing %>%
  bind_cols(predict(arabiensis_stack, ., type="prob"))
```

And look at the goodness of fit with AUC

```{r assess_test}
roc_auc(data= arabiensis_test_pred,truth=class,.pred_presence)
```

# Projecting to the present
```{r plot_present}
prediction_present <- predict_raster(arabiensis_stack, climate_present, type="prob")
ggplot() +
  geom_spatraster(data=prediction_present, aes(fill=.pred_presence))+
  scale_fill_terrain_c() +
  geom_sf(data = arabiensis %>% filter(class=="presence"))
```


# Projecting to other times

We will project the leopard range to a different time. We will get the climate
for the Last Glacial Maximum, using the `pastclim` package.

```{r get_lgm}
climate_lgm<-pastclim::region_slice(time_bp = -21000, 
                                        bio_variables = vars_to_keep, 
                                        data="Beyer2020", 
                                        crop=region_outline$Africa)
```

And predict using the ensemble:
```{r plot_lgm}
prediction_lgm <- predict_raster(arabiensis_stack, climate_lgm, type="prob")
ggplot() +
  geom_spatraster(data=prediction_lgm, aes(fill=.pred_presence))+
  scale_fill_terrain_c()
```
